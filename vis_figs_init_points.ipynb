{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import concurrent.futures\n",
    "import timeit\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "from functools import partial\n",
    "import copy\n",
    "from itertools import groupby\n",
    "import sys, os\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import (\n",
    "    cosine\n",
    ")\n",
    "\n",
    "from data_loader import load_grid_search_data, get_recon_landscape, get_recon_pathname\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = (15.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _vis_landscapes(\n",
    "        landscapes, # list of np.ndarray\n",
    "        labels, # list of labels of correlated landscapes\n",
    "        full_range, # dict, \n",
    "        true_optima,\n",
    "        title,\n",
    "        save_path, # figure save path\n",
    "        params_paths, # list of list of parameters correlated to landscapes\n",
    "        recon_params_path_dict=None,\n",
    "        origin_params_path_dict=None,\n",
    "        converged_soln_markers=None,\n",
    "    ):\n",
    "\n",
    "    assert len(landscapes) == len(labels)\n",
    "    assert len(landscapes) == len(params_paths)\n",
    "\n",
    "    # plt.figure\n",
    "    plt.rc('font', size=28)\n",
    "    if len(landscapes) == 2:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(30, 10))\n",
    "    elif len(landscapes) == 3:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(30, 10))\n",
    "    elif len(landscapes) == 4:\n",
    "        fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(30, 30))\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    axs = axs.reshape(-1)\n",
    "\n",
    "    # TODO Check ij and xy\n",
    "    X, Y = np.meshgrid(full_range['beta'], full_range['gamma'])\n",
    "\n",
    "    # c = ax.pcolormesh(X, Y, Z, cmap='viridis', vmin=Z.min(), vmax=Z.max())\n",
    "    for idx, landscape in enumerate(landscapes):\n",
    "        im = axs[idx].pcolormesh(X, Y, landscape) #, cmap='viridis', vmin=origin.min(), vmax=origin.max())\n",
    "        axs[idx].set_title(labels[idx])\n",
    "        axs[idx].set_xlabel('beta')\n",
    "        axs[idx].set_ylabel('gamma')\n",
    "        if isinstance(true_optima, list) or isinstance(true_optima, np.ndarray):\n",
    "            axs[idx].plot(true_optima[1], true_optima[0], marker=\"o\", color='red', markersize=7, label=\"true optima\")\n",
    "\n",
    "        params = params_paths[idx]\n",
    "        if isinstance(params, list) or isinstance(params, np.ndarray):\n",
    "            xs = [] # beta\n",
    "            ys = [] # gamma\n",
    "            for param in params:\n",
    "                xs.append(param[1])\n",
    "                ys.append(param[0])\n",
    "\n",
    "            axs[idx].plot(xs, ys, linewidth=3, color='purple', label=\"optimization path\")\n",
    "            axs[idx].plot(xs[0], ys[0], marker=\"o\", color='white', markersize=9, label=\"initial point\")\n",
    "\n",
    "            if isinstance(converged_soln_markers, list):\n",
    "                marker = converged_soln_markers[idx]\n",
    "                if marker == '*':\n",
    "                    markersize = 15\n",
    "                elif marker == 'D':\n",
    "                    markersize = 10\n",
    "                \n",
    "            else:\n",
    "                marker = '*'\n",
    "                markersize = 15\n",
    "\n",
    "            axs[idx].plot(xs[-1], ys[-1], marker=marker,\n",
    "                    color='red', markersize=markersize, label=\"converged solution\")\n",
    "\n",
    "            axs[idx].legend()\n",
    "\n",
    "    fig.colorbar(im, ax=[axs[i] for i in range(len(landscapes))])\n",
    "    # plt.legend()\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_loader import get_recon_landscape, get_recon_pathname\n",
    "\n",
    "\n",
    "def vis_find_init_pts_by_recon(n_qubits: int, eps: float, noise: str):\n",
    "    \"\"\"n=20\n",
    "    \"\"\"\n",
    "    n = n_qubits\n",
    "    method = 'sv'\n",
    "    problem = 'maxcut'\n",
    "    cs_seed = n_qubits\n",
    "    p = 2\n",
    "\n",
    "    sf = 0.05\n",
    "    seed = 0\n",
    "    if p == 2:\n",
    "        bs = 12\n",
    "        gs = 15\n",
    "    elif p == 1:\n",
    "        bs = 50\n",
    "        gs = 100\n",
    "    else:\n",
    "        raise ValueError(\"Invalid depth of QAOA\")\n",
    "\n",
    "    data, data_fname, _ = load_grid_search_data(\n",
    "        n_qubits=n_qubits, p=p, problem=problem, method=method,\n",
    "        noise=noise, beta_step=bs, gamma_step=gs, seed=seed\n",
    "    )\n",
    "    origin = data['data']\n",
    "\n",
    "    recon_path, _, _ = get_recon_pathname(p, problem, method, noise, cs_seed, sf, data_fname)\n",
    "    # recon1_path = f\"figs/recon_p2_landscape/{timestamp}/recon-sf={sf:.3f}-cs_seed={cs_seed}-{miti1_data_fname}\"\n",
    "    recon = get_recon_landscape(p, origin, sf, False, recon_path, cs_seed)\n",
    "\n",
    "    # recon_path = f\"figs/gen_p1_landscape/sv-ideal/2D_CS_recon_p2/sf{sf:.3f}_bs{bs}_gs{gs}_nQ{n_qubits}_seed{seed}_csSeed{cs_seed}.npz\"\n",
    "    # recon = np.load(f\"{recon_path}\")['recon']\n",
    "\n",
    "    # ====================== vis ======================\n",
    "    # eps = 0.6\n",
    "    if noise == 'ideal' and n == 16:\n",
    "        init_data = np.load(\n",
    "            \"figs/find_init_pts_by_recon/2022-11-11_23:23:40/init-maxiter=None-eps=0.400-sv-ideal-n=16-p=2-seed=0-12-15.npz\"\n",
    "            # \"figs/find_init_pts_by_recon/2022-11-05_22:03:07_OK/recon-eps=0.600-csSeed=16-sv-ideal-n=16-p=2-seed=0-12-15.npz\"\n",
    "            , allow_pickle=True)\n",
    "        C_opt = -17.991035 # 当时忘记存了\n",
    "        offset = -12\n",
    "        \n",
    "        \n",
    "    elif noise == 'depolar-0.001-0.02' and n == 16:\n",
    "        raise NotImplementedError(\"优化不动\")\n",
    "\n",
    "    elif noise == 'ideal' and n == 20:\n",
    "        init_data_path = \"figs/find_init_pts_by_recon/2022-11-11_14:13:36_OK/init-maxiter=None-eps=0.600-sv-ideal-n=20-p=2-seed=0-12-15.npz\"\n",
    "        print(\"init data path =\", init_data_path)\n",
    "        init_data = np.load(init_data_path, allow_pickle=True)\n",
    "        offset = init_data['offset']\n",
    "        C_opt = init_data['C_opt']\n",
    "        \n",
    "    elif noise == 'depolar-0.001-0.02' and n == 20:\n",
    "        raise NotImplementedError(\"优化不动\")\n",
    "    \n",
    "    ids = init_data['ids']\n",
    "    energies = init_data['min_energies']\n",
    "    min_recon = init_data['min_recon']\n",
    "    initial_points = init_data['initial_points']\n",
    "    recon_init_pt_vals = init_data['recon_init_pt_vals']\n",
    "    # C_opt = init_data['C_opt']\n",
    "    # offset = init_data['offset']\n",
    "    # eps = init_data['eps']\n",
    "    print(\"C_opt:\", C_opt)\n",
    "    print(\"offset:\", offset)\n",
    "    print(\"eps:\", init_data['eps'])\n",
    "\n",
    "\n",
    "    if n == 16:\n",
    "        oscar_energies = init_data['min_energies'], # one to one correspondence to ids\n",
    "        oscar_energies = oscar_energies[0]\n",
    "        oscar_init = init_data['initial_points'], # one to one correspondence to ids\n",
    "        oscar_rsts = init_data['oscar_rsts']\n",
    "\n",
    "        random_rsts = init_data['random_rsts']\n",
    "        random_inits = init_data['random_inits']\n",
    "        random_energies = np.array([r[0] for r in random_rsts])\n",
    "\n",
    "        ids_higher_bp = np.array(\n",
    "            [1, 2, 3, 4, 9, 10]\n",
    "        )\n",
    "    \n",
    "        print(random_energies)\n",
    "\n",
    "        print(oscar_energies)\n",
    "\n",
    "        print(\"OSCAR mean:\", oscar_energies.mean(), \"RANDOM mean:\", random_energies.mean())\n",
    "        print(\"OSCAR mean:\", oscar_energies[ids_higher_bp].mean(), \"RANDOM mean:\", random_energies[ids_higher_bp].mean())\n",
    "\n",
    "    # return\n",
    "\n",
    "    # C_opt = -17.991035\n",
    "    # offset = -12\n",
    "\n",
    "    # energies = np.array(energies, dtype=float)\n",
    "    # energies += offset\n",
    "\n",
    "    # recon. error on the initial points\n",
    "    recon_errors = []\n",
    "    ids_needed = []\n",
    "    for i in range(len(ids)): # idx: tuples\n",
    "        # print(recon[idx[0], idx[1], idx[2], idx[3]])\n",
    "        # print(recon[tuple(idx)])\n",
    "        # print(idx, recon[idx[0], idx[1], idx[2], idx[3]])\n",
    "        # continue\n",
    "\n",
    "        init_recon = recon[tuple(ids[i])]\n",
    "        init_origin = origin[tuple(ids[i])]\n",
    "        \n",
    "        if np.abs(min_recon - init_recon) >= eps:\n",
    "            continue\n",
    "\n",
    "        error = init_recon - init_origin\n",
    "        # error = np.abs(init_recon - init_origin)\n",
    "        error = np.abs(error)\n",
    "        # print(init_origin, init_recon, error)\n",
    "        recon_errors.append(error)\n",
    "        ids_needed.append(i)\n",
    "\n",
    "    ids_needed = np.array(ids_needed)\n",
    "    # print(ids_needed)\n",
    "    energies = energies[ids_needed]\n",
    "\n",
    "    # ==================== vis =====================\n",
    "    import matplotlib.patches as mpatches\n",
    "    diff = C_opt - energies\n",
    "    diff = energies - C_opt\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    axs.reshape(-1)\n",
    "    # plt.hist(diff, bins=50)\n",
    "    # https://blog.csdn.net/mighty13/article/details/117405722\n",
    "    rst = axs[0].violinplot(diff, quantiles=[0.25, 0.5, 0.75], vert=False)\n",
    "\n",
    "    # color = rst['bodies'][0].get_facecolor().flatten()\n",
    "    # patch = mpatches.Patch(color=color, label=\"\")\n",
    "    axs[0].set_xlabel(\"optimized optima $-$ true optima\")\n",
    "    # axs.set_ylabel(\"# initial points\")\n",
    "    axs[0].set_ylabel(\"Kernel density estimation (KDE)\")\n",
    "    # plt.legend()\n",
    "    axs[0].set_title(f\"total {len(ids_needed)} initial points, epsilon: {eps:.2f}\")\n",
    "    print(rst)\n",
    "    print(rst['cquantiles'])\n",
    "\n",
    "    # ============ correlation value ==========\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    r_p, p_p = pearsonr(diff, recon_errors)\n",
    "    r_s, p_s = spearmanr(diff, recon_errors)\n",
    "    print(\"\")\n",
    "    \n",
    "    print(f\"Pearson : r={r_p:+.5f}\\t p={p_p:.5f}\\n\"\n",
    "          f\"Spearman: r={r_s:+.5f}\\t p={p_s:.5f}\")\n",
    "    \n",
    "    axs[1].scatter(diff, recon_errors)\n",
    "\n",
    "    # color = rst['bodies'][0].get_facecolor().flatten()\n",
    "    # patch = mpatches.Patch(color=color, label=\"\")\n",
    "    axs[1].set_xlabel(\"optimized optima $-$ true optima\")\n",
    "    # axs.set_ylabel(\"# initial points\")\n",
    "    axs[1].set_ylabel(\"f_recon - f_origin, on init point\")\n",
    "    # plt.legend()\n",
    "    axs[1].set_title(f\"total {len(ids_needed)} initial points, epsilon: {eps:.2f}\")\n",
    "    \n",
    "    # plt.title(f\"# qubits={n_qubits}, p={p}\")\n",
    "\n",
    "    # quantiles\n",
    "\n",
    "\n",
    "vis_find_init_pts_by_recon(n_qubits=16, eps=0.6, noise=\"ideal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_loader import get_recon_landscape, get_recon_pathname\n",
    "\n",
    "\n",
    "def vis_find_init_pts_by_recon(n_qubits: int, eps: float, noise: str):\n",
    "    \"\"\"n=20\n",
    "    \"\"\"\n",
    "    n = n_qubits\n",
    "    method = 'sv'\n",
    "    problem = 'maxcut'\n",
    "    cs_seed = n_qubits\n",
    "    p = 2\n",
    "\n",
    "    sf = 0.05\n",
    "    seed = 0\n",
    "    if p == 2:\n",
    "        bs = 12\n",
    "        gs = 15\n",
    "    elif p == 1:\n",
    "        bs = 50\n",
    "        gs = 100\n",
    "    else:\n",
    "        raise ValueError(\"Invalid depth of QAOA\")\n",
    "\n",
    "    data, data_fname, _ = load_grid_search_data(\n",
    "        n_qubits=n_qubits, p=p, problem=problem, method=method,\n",
    "        noise=noise, beta_step=bs, gamma_step=gs, seed=seed\n",
    "    )\n",
    "    origin = data['data']\n",
    "\n",
    "    recon_path, _, _ = get_recon_pathname(p, problem, method, noise, cs_seed, sf, data_fname)\n",
    "    # recon1_path = f\"figs/recon_p2_landscape/{timestamp}/recon-sf={sf:.3f}-cs_seed={cs_seed}-{miti1_data_fname}\"\n",
    "    recon = get_recon_landscape(p, origin, sf, False, recon_path, cs_seed)\n",
    "\n",
    "    # recon_path = f\"figs/gen_p1_landscape/sv-ideal/2D_CS_recon_p2/sf{sf:.3f}_bs{bs}_gs{gs}_nQ{n_qubits}_seed{seed}_csSeed{cs_seed}.npz\"\n",
    "    # recon = np.load(f\"{recon_path}\")['recon']\n",
    "\n",
    "    # ====================== vis ======================\n",
    "    # eps = 0.6\n",
    "    if noise == 'ideal' and n == 16:\n",
    "        init_data = np.load(\n",
    "            \"figs/find_init_pts_by_recon/2022-11-05_22:03:07_OK/recon-eps=0.600-csSeed=16-sv-ideal-n=16-p=2-seed=0-12-15.npz\",\n",
    "            allow_pickle=True)\n",
    "        C_opt = -17.991035 # 当时忘记存了\n",
    "        offset = -12\n",
    "        \n",
    "    elif noise == 'depolar-0.001-0.02' and n == 16:\n",
    "        raise NotImplementedError(\"优化不动\")\n",
    "\n",
    "    elif noise == 'ideal' and n == 20:\n",
    "        init_data_path = \"figs/find_init_pts_by_recon/2022-11-11_14:13:36_OK/init-maxiter=None-eps=0.600-sv-ideal-n=20-p=2-seed=0-12-15.npz\"\n",
    "        print(\"init data path =\", init_data_path)\n",
    "        init_data = np.load(init_data_path, allow_pickle=True)\n",
    "        offset = init_data['offset']\n",
    "        C_opt = init_data['C_opt']\n",
    "        \n",
    "    elif noise == 'depolar-0.001-0.02' and n == 20:\n",
    "        raise NotImplementedError(\"优化不动\")\n",
    "    \n",
    "    ids = init_data['ids']\n",
    "    energies = init_data['min_energies']\n",
    "    min_recon = init_data['min_recon']\n",
    "    initial_points = init_data['initial_points']\n",
    "    recon_init_pt_vals = init_data['recon_init_pt_vals']\n",
    "    # C_opt = -17.991035\n",
    "    # offset = -12\n",
    "    # eps = init_data['eps']\n",
    "\n",
    "    # print(ids)\n",
    "    # print(energies)\n",
    "    energies = np.array(energies, dtype=float)\n",
    "    # energies += offset\n",
    "    # print(energies)\n",
    "\n",
    "    # recon. error on the initial points\n",
    "    recon_errors = []\n",
    "    ids_needed = []\n",
    "    for i in range(len(ids)): # idx: tuples\n",
    "        # print(recon[idx[0], idx[1], idx[2], idx[3]])\n",
    "        # print(recon[tuple(idx)])\n",
    "        # print(idx, recon[idx[0], idx[1], idx[2], idx[3]])\n",
    "        # continue\n",
    "\n",
    "        init_recon = recon[tuple(ids[i])]\n",
    "        init_origin = origin[tuple(ids[i])]\n",
    "        \n",
    "        if np.abs(min_recon - init_recon) >= eps:\n",
    "            continue\n",
    "\n",
    "\n",
    "        error = init_recon - init_origin\n",
    "        # error = np.abs(init_recon - init_origin)\n",
    "        error = np.abs(error)\n",
    "        # print(init_origin, init_recon, error)\n",
    "        recon_errors.append(error)\n",
    "        ids_needed.append(i)\n",
    "\n",
    "    ids_needed = np.array(ids_needed)\n",
    "    # print(ids_needed)\n",
    "    energies = energies[ids_needed]\n",
    "    print(\"mean min_energies:\", energies.mean())\n",
    "\n",
    "    ids_higher_bp = [9, 10, 12, 14, 16, 56, 57, 58, 59]\n",
    "    ids_higher_bp = [9, 10, 12, 14, 16, 53, 56, 57, 58, 59] # stride=8, 结果变好\n",
    "    ids_higher_bp = [9, 10, 12, 14, 16, 24, 25, 26, 27, 28, 29, 32, 33, 34, 36, 37, 38, 39, 40, 41, 47, 53, 56, 58, 59] # stride=9，结果变坏\n",
    "    ids_higher_bp = np.array(ids_higher_bp)\n",
    "    print(\"mean filtered_energies:\", energies[ids_higher_bp].mean())\n",
    "\n",
    "    # ==================== vis =====================\n",
    "    import matplotlib.patches as mpatches\n",
    "    diff = C_opt - energies\n",
    "    diff = energies - C_opt\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    axs.reshape(-1)\n",
    "    # plt.hist(diff, bins=50)\n",
    "    # https://blog.csdn.net/mighty13/article/details/117405722\n",
    "    rst = axs[0].violinplot(diff, quantiles=[0.25, 0.5, 0.75], vert=False)\n",
    "\n",
    "    # color = rst['bodies'][0].get_facecolor().flatten()\n",
    "    # patch = mpatches.Patch(color=color, label=\"\")\n",
    "    axs[0].set_xlabel(\"optimized optima $-$ true optima\")\n",
    "    # axs.set_ylabel(\"# initial points\")\n",
    "    axs[0].set_ylabel(\"Kernel density estimation (KDE)\")\n",
    "    # plt.legend()\n",
    "    axs[0].set_title(f\"total {len(ids_needed)} initial points, epsilon: {eps:.2f}\")\n",
    "    print(rst)\n",
    "    print(rst['cquantiles'])\n",
    "\n",
    "    # ============ correlation value ==========\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    r_p, p_p = pearsonr(diff, recon_errors)\n",
    "    r_s, p_s = spearmanr(diff, recon_errors)\n",
    "    print(\"\")\n",
    "    \n",
    "    print(f\"Pearson : r={r_p:+.5f}\\t p={p_p:.5f}\\n\"\n",
    "          f\"Spearman: r={r_s:+.5f}\\t p={p_s:.5f}\")\n",
    "    \n",
    "    axs[1].scatter(diff, recon_errors)\n",
    "\n",
    "    # color = rst['bodies'][0].get_facecolor().flatten()\n",
    "    # patch = mpatches.Patch(color=color, label=\"\")\n",
    "    axs[1].set_xlabel(\"optimized optima $-$ true optima\")\n",
    "    # axs.set_ylabel(\"# initial points\")\n",
    "    axs[1].set_ylabel(\"f_recon - f_origin, on init point\")\n",
    "    # plt.legend()\n",
    "    axs[1].set_title(f\"total {len(ids_needed)} initial points, epsilon: {eps:.2f}\")\n",
    "    \n",
    "    # plt.title(f\"# qubits={n_qubits}, p={p}\")\n",
    "\n",
    "    # quantiles\n",
    "\n",
    "\n",
    "vis_find_init_pts_by_recon(n_qubits=16, eps=0.6, noise=\"ideal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "read grid search data from figs/grid_search/maxcut/sv-ideal-p=1/maxcut-sv-ideal-n=8-p=1-seed=0-50-100.npz\n",
      "\n",
      "recon landscape read from figs/grid_search_recon/maxcut/sv-ideal-p=1/recon-cs_seed=8-sf=0.050-maxcut-sv-ideal-n=8-p=1-seed=0-50-100.npz\n",
      "dict_keys(['eps', 'C_opt', 'offset', 'maxiter', 'recon_init_pt_vals', 'origin_init_pt_vals', 'min_recon', 'oscar_rsts', 'oscar_mins', 'oscar_inits', 'oscar_init_ids', 'random_mins', 'random_rsts', 'random_inits', 'ids_higher_bp'])\n",
      "-1.9516601562500002 -1.9379882812499998\n",
      "avg_min_loss_random = -1.9379882812499998\n",
      "avg_min_loss_oscar = -1.9516601562500002\n"
     ]
    }
   ],
   "source": [
    "def vis_compare_oscar_and_random(n_qubits: int, p: int, eps: float, noise: str):\n",
    "    n = n_qubits\n",
    "    method = 'sv'\n",
    "    problem = 'maxcut'\n",
    "    cs_seed = n_qubits\n",
    "\n",
    "    sf = 0.05\n",
    "    seed = 0\n",
    "    if p == 2:\n",
    "        bs = 12\n",
    "        gs = 15\n",
    "    elif p == 1:\n",
    "        bs = 50\n",
    "        gs = 100\n",
    "    else:\n",
    "        raise ValueError(\"Invalid depth of QAOA\")\n",
    "\n",
    "    data, data_fname, _ = load_grid_search_data(\n",
    "        n_qubits=n_qubits, p=p, problem=problem, method=method,\n",
    "        noise=noise, beta_step=bs, gamma_step=gs, seed=seed\n",
    "    )\n",
    "    origin = data['data']\n",
    "\n",
    "    recon_path, _, _ = get_recon_pathname(p, problem, method, noise, cs_seed, sf, data_fname)\n",
    "    # recon1_path = f\"figs/recon_p2_landscape/{timestamp}/recon-sf={sf:.3f}-cs_seed={cs_seed}-{miti1_data_fname}\"\n",
    "    recon = get_recon_landscape(p, origin, sf, False, recon_path, cs_seed)\n",
    "\n",
    "    # recon_path = f\"figs/gen_p1_landscape/sv-ideal/2D_CS_recon_p2/sf{sf:.3f}_bs{bs}_gs{gs}_nQ{n_qubits}_seed{seed}_csSeed{cs_seed}.npz\"\n",
    "    # recon = np.load(f\"{recon_path}\")['recon']\n",
    "\n",
    "    # ====================== vis ======================\n",
    "    # eps = 0.6\n",
    "\n",
    "    if noise == 'ideal' and n == 20:\n",
    "        init_data_path = \"figs/find_init_pts_by_recon/2022-11-11_14:13:36_OK/init-maxiter=None-eps=0.600-sv-ideal-n=20-p=2-seed=0-12-15.npz\"\n",
    "        print(\"init data path =\", init_data_path)\n",
    "        # print(init_data_path)\n",
    "        init_data = np.load(init_data_path, allow_pickle=True)\n",
    "    elif n == 8:\n",
    "        init_data = np.load(\n",
    "            \"figs/find_init_pts_by_recon/2022-11-19_21:16:48/init-maxiter=None-eps=[0.01]-maxcut-sv-ideal-n=8-p=1-seed=0-50-100.npz\"\n",
    "            , allow_pickle=True\n",
    "        )\n",
    "    elif noise == 'depolar-0.001-0.02' and n == 20:\n",
    "        raise NotImplementedError(\"优化不动\")\n",
    "\n",
    "    # ----------------- extract information ----------------------\n",
    "    init_data = dict(init_data)\n",
    "    print(init_data.keys())\n",
    "    \n",
    "    def extract_opt_data(d: dict, label: str):\n",
    "        return d[f'{label}_rsts'], d[f'{label}_mins'], d[f'{label}_inits']\n",
    "\n",
    "\n",
    "    oscar_rsts, oscar_mins, oscar_inits = extract_opt_data(init_data, \"oscar\")\n",
    "    random_rsts, random_mins, random_inits = extract_opt_data(init_data, \"random\")\n",
    "\n",
    "    oscar_init_ids = init_data['oscar_init_ids']\n",
    "    recon_init_pt_vals = init_data['recon_init_pt_vals']\n",
    "    origin_init_pt_vals = init_data['origin_init_pt_vals']\n",
    "    offset = init_data['offset']\n",
    "    C_opt = init_data['C_opt']\n",
    "    # eps = init_data['eps']\n",
    "    # print(f\"offset: {offset}, eps: {eps:.3f}\")\n",
    "    # assert C_opt == None\n",
    "\n",
    "    print(oscar_mins.mean(), random_mins.mean())\n",
    "\n",
    "    avg_min_loss_random = np.mean(random_mins)\n",
    "    avg_min_loss_oscar = np.mean(oscar_mins)\n",
    "\n",
    "    print(\"avg_min_loss_random =\", avg_min_loss_random)\n",
    "    print(\"avg_min_loss_oscar =\", avg_min_loss_oscar)\n",
    "\n",
    "    # ids_higher_bp = [12, 13, 15, 16]\n",
    "    # ids_higher_bp = np.array(ids_higher_bp)\n",
    "    \n",
    "    # print(\"avg_min_loss_random =\", random_mins[ids_higher_bp].mean())\n",
    "    # print(\"avg_min_loss_oscar =\", oscar_mins[ids_higher_bp].mean())\n",
    "\n",
    "\n",
    "    # # recon. error on the initial points\n",
    "    is_filter = False\n",
    "    if is_filter:\n",
    "        recon_errors = []\n",
    "        ids_needed = []\n",
    "        for i in range(len(ids)): # idx: tuples\n",
    "            # print(recon[idx[0], idx[1], idx[2], idx[3]])\n",
    "            # print(recon[tuple(idx)])\n",
    "            # print(idx, recon[idx[0], idx[1], idx[2], idx[3]])\n",
    "            # continue\n",
    "\n",
    "            init_recon = recon[tuple(ids[i])]\n",
    "            init_origin = origin[tuple(ids[i])]\n",
    "            \n",
    "            if np.abs(min_recon - init_recon) >= eps:\n",
    "                continue\n",
    "\n",
    "            error = init_recon - init_origin\n",
    "            # error = np.abs(init_recon - init_origin)\n",
    "            error = np.abs(error)\n",
    "            # print(init_origin, init_recon, error)\n",
    "            recon_errors.append(error)\n",
    "            ids_needed.append(i)\n",
    "\n",
    "        ids_needed = np.array(ids_needed)\n",
    "        # print(ids_needed)\n",
    "        filtered_energies = oscar_energies[ids_needed]\n",
    "        filtered_mean = np.array(filtered_energies).mean()\n",
    "        n_filtered = len(ids_needed)\n",
    "        print(f\"epsilon: {eps}, n pts:\", n_filtered)\n",
    "\n",
    "        print(f\"OSCAR_mean: {filtered_mean}; RANDOM_mean: {random_energies[:n_filtered].mean()}\")\n",
    "\n",
    "    # # ==================== vis =====================\n",
    "    # import matplotlib.patches as mpatches\n",
    "    # diff = C_opt - energies\n",
    "    # diff = energies - C_opt\n",
    "    # fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    # axs.reshape(-1)\n",
    "    # # plt.hist(diff, bins=50)\n",
    "    # # https://blog.csdn.net/mighty13/article/details/117405722\n",
    "    # rst = axs[0].violinplot(diff, quantiles=[0.25, 0.5, 0.75], vert=False)\n",
    "\n",
    "    # # color = rst['bodies'][0].get_facecolor().flatten()\n",
    "    # # patch = mpatches.Patch(color=color, label=\"\")\n",
    "    # axs[0].set_xlabel(\"optimized optima $-$ true optima\")\n",
    "    # # axs.set_ylabel(\"# initial points\")\n",
    "    # axs[0].set_ylabel(\"Kernel density estimation (KDE)\")\n",
    "    # # plt.legend()\n",
    "    # axs[0].set_title(f\"total {len(ids_needed)} initial points, epsilon: {eps:.2f}\")\n",
    "    # print(rst)\n",
    "    # print(rst['cquantiles'])\n",
    "\n",
    "    # # ============ correlation value ==========\n",
    "    # from scipy.stats import pearsonr, spearmanr\n",
    "    # r_p, p_p = pearsonr(diff, recon_errors)\n",
    "    # r_s, p_s = spearmanr(diff, recon_errors)\n",
    "    # print(\"\")\n",
    "    \n",
    "    # print(f\"Pearson : r={r_p:+.5f}\\t p={p_p:.5f}\\n\"\n",
    "    #       f\"Spearman: r={r_s:+.5f}\\t p={p_s:.5f}\")\n",
    "    \n",
    "    # axs[1].scatter(diff, recon_errors)\n",
    "\n",
    "    # # color = rst['bodies'][0].get_facecolor().flatten()\n",
    "    # # patch = mpatches.Patch(color=color, label=\"\")\n",
    "    # axs[1].set_xlabel(\"optimized optima $-$ true optima\")\n",
    "    # # axs.set_ylabel(\"# initial points\")\n",
    "    # axs[1].set_ylabel(\"f_recon - f_origin, on init point\")\n",
    "    # # plt.legend()\n",
    "    # axs[1].set_title(f\"total {len(ids_needed)} initial points, epsilon: {eps:.2f}\")\n",
    "    \n",
    "    # plt.title(f\"# qubits={n_qubits}, p={p}\")\n",
    "\n",
    "    # quantiles\n",
    "# vis_compare_oscar_and_random(n_qubits=20, eps=0.6, noise=\"ideal\")\n",
    "# vis_compare_oscar_and_random(n_qubits=16, eps=0.6, noise=\"ideal\")\n",
    "vis_compare_oscar_and_random(n_qubits=8, p=1, eps=0.6, noise=\"ideal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only once\n",
    "def merge_n20_data():\n",
    "    # random\n",
    "    d1 = np.load(\n",
    "        \"figs/find_init_pts_by_recon/2022-11-11_13:57:36_random/init-maxiter=None-eps=0.600-sv-ideal-n=20-p=2-seed=0-12-15.npz\"\n",
    "        , allow_pickle=True\n",
    "    )\n",
    "\n",
    "    # oscar\n",
    "    d2 = np.load(\n",
    "        \"figs/find_init_pts_by_recon/2022-11-11_00:29:27_oscar/init-maxiter=None-eps=0.600-sv-ideal-n=20-p=2-seed=0-12-15.npz\"\n",
    "        , allow_pickle=True\n",
    "    )\n",
    "\n",
    "    d1 = dict(d1)\n",
    "    print(d1)\n",
    "    d2 = dict(d2)\n",
    "    print(d2)\n",
    "\n",
    "    print(\"d1 =\", d1.keys())\n",
    "    print(\"d2 =\", d2.keys())\n",
    "    \n",
    "    # print(d1['C_opt'])\n",
    "    # print(d1['offset'])\n",
    "    # print(d2['min_energies'])\n",
    "    \n",
    "    d1['random_energies'] = d2['random_energies']\n",
    "    d1['random_inits'] = d2['random_inits']\n",
    "    d1['maxiter'] = d2['maxiter']\n",
    "\n",
    "    print(d1.keys())\n",
    "\n",
    "    # print(d1)\n",
    "    # d1 = d2\n",
    "\n",
    "    save_path = \"figs/find_init_pts_by_recon/2022-11-11_14:13:36_OK/init-maxiter=None-eps=0.600-sv-ideal-n=20-p=2-seed=0-12-15.npz\"\n",
    "    np.savez_compressed(\n",
    "        save_path,\n",
    "        ids=d1['ids'],\n",
    "        initial_points=d1['initial_points'], # one to one correspondence to ids\n",
    "        min_energies=d1['min_energies'], # one to one correspondence to ids\n",
    "        min_recon=d1['min_recon'],\n",
    "        recon_init_pt_vals=d1['recon_init_pt_vals'],\n",
    "        eps=d1['eps'],\n",
    "        C_opt=d1['C_opt'],\n",
    "        offset=d1['offset'],\n",
    "        maxiter=d1['maxiter'],\n",
    "\n",
    "        # diff\n",
    "        random_energies=d1['random_energies'],\n",
    "        random_inits=d1['random_inits'],\n",
    "    )\n",
    "\n",
    "    d = np.load(\n",
    "        save_path,\n",
    "        allow_pickle=True\n",
    "    )\n",
    "    d = dict(d)\n",
    "    print(d.keys())\n",
    "    \n",
    "\n",
    "merge_n20_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_diff_eps_data():\n",
    "    paths = [\n",
    "        \"figs/find_init_pts_by_recon/2022-11-15_02:57:13/init-maxiter=None-eps=[0.4]-sv-ideal-n=16-p=2-seed=0-12-15.npz\"\n",
    "        ,\n",
    "        \"figs/find_init_pts_by_recon/2022-11-15_02:48:16/init-maxiter=None-eps=[0.4, 0.45]-sv-ideal-n=16-p=2-seed=0-12-15.npz\"\n",
    "        ,\n",
    "        \"figs/find_init_pts_by_recon/2022-11-15_03:07:24/init-maxiter=None-eps=[0.45, 0.5]-sv-ideal-n=16-p=2-seed=0-12-15.npz\"\n",
    "    ]\n",
    "\n",
    "    init_datas = []\n",
    "    for path in paths:\n",
    "        init_datas.append(np.load(path, allow_pickle=True))\n",
    "\n",
    "    oscar_energies = []\n",
    "    oscar_inits = []\n",
    "    random_energies = []\n",
    "    for data in init_datas:\n",
    "        oscar_energies.append(data['oscar_energies'])\n",
    "        # min_energies,\n",
    "        # oscar_inits=ids, # indices\n",
    "\n",
    "        # random initialization method\n",
    "        # random_energies=random_energies,\n",
    "        # random_rsts=random_rsts,\n",
    "        random_rsts = data['random_rsts']\n",
    "        random_energies.append(\n",
    "            np.array([r[0] for r in random_rsts])\n",
    "        )\n",
    "    \n",
    "    oscar_energies = np.concatenate(oscar_energies)\n",
    "    random_energies = np.concatenate(random_energies)\n",
    "\n",
    "    print(oscar_energies.shape)\n",
    "    print(random_energies.shape)\n",
    "\n",
    "    print(oscar_energies.mean(), random_energies.mean())\n",
    "    print(oscar_energies.min(), random_energies.min())\n",
    "    return\n",
    "merge_diff_eps_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39_mitiq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96fb422eda4202d64a5d32f92bf4bc9789b7b1bb6dc4fd0c3326766fb0959649"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
